{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model # PEFT model\n",
    "import torch  # PyTorch, needless to say ;)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig  # Main components of LLM\n",
    "from pathlib import Path # For file paths\n",
    "from datasets import load_dataset, Dataset # For loading the dataset\n",
    "from trl import SFTTrainer  # An easy-to-use trainer\n",
    "import pandas  # For loading and processing the dataset\n",
    "import random  # Sampling random records from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['中華料理的菜色',\n",
       " '如星星一般千變萬化',\n",
       " '而今擴展到全世界',\n",
       " '令人驚異的料理',\n",
       " '中華料理',\n",
       " '中華料理，經歷四千年',\n",
       " '分爲幾個大宗',\n",
       " '最後達到前所未有之大成',\n",
       " '亦達到其顚峰',\n",
       " '當時出現好幾位',\n",
       " '手藝驚人的料理師傅激烈對抗']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = load_dataset(\"h-alice/cooking-master-boy-subtitle\", split=\"train\")\n",
    "dataset_raw[\"caption\"][0:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking massages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>next_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中華料理的菜色</td>\n",
       "      <td>如星星一般千變萬化</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>如星星一般千變萬化</td>\n",
       "      <td>而今擴展到全世界</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>而今擴展到全世界</td>\n",
       "      <td>令人驚異的料理</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>令人驚異的料理</td>\n",
       "      <td>中華料理</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>中華料理</td>\n",
       "      <td>中華料理，經歷四千年</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     message next_message\n",
       "0    中華料理的菜色    如星星一般千變萬化\n",
       "1  如星星一般千變萬化     而今擴展到全世界\n",
       "2   而今擴展到全世界      令人驚異的料理\n",
       "3    令人驚異的料理         中華料理\n",
       "4       中華料理   中華料理，經歷四千年"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ds = dataset_raw.to_pandas()\n",
    "\n",
    "\n",
    "indicies = [i for i in range(0, len(temp_ds)-1)]\n",
    "next_indicies = [i+1 for i in range(0, len(temp_ds)-1)]\n",
    "\n",
    "\n",
    "\n",
    "# Fetch records.\n",
    "temp_message_stack = pandas.DataFrame()\n",
    "\n",
    "temp_message_stack[\"message\"] = temp_ds.iloc[indicies][\"caption\"].reset_index(drop=True)\n",
    "temp_message_stack[\"next_message\"] = temp_ds.iloc[next_indicies][\"caption\"].reset_index(drop=True)\n",
    "\n",
    "temp_message_stack.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucions format for Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMMA_INST = \"<bos><start_of_turn>user\\n{full_user_prompt}<end_of_turn>\\n<start_of_turn>model\\n{model}<eos>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gemma instrutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': ['<bos><start_of_turn>user\\n中華料理的菜色<end_of_turn>\\n<start_of_turn>model\\n如星星一般千變萬化<eos>',\n",
       "  '<bos><start_of_turn>user\\n如星星一般千變萬化<end_of_turn>\\n<start_of_turn>model\\n而今擴展到全世界<eos>',\n",
       "  '<bos><start_of_turn>user\\n而今擴展到全世界<end_of_turn>\\n<start_of_turn>model\\n令人驚異的料理<eos>',\n",
       "  '<bos><start_of_turn>user\\n令人驚異的料理<end_of_turn>\\n<start_of_turn>model\\n中華料理<eos>',\n",
       "  '<bos><start_of_turn>user\\n中華料理<end_of_turn>\\n<start_of_turn>model\\n中華料理，經歷四千年<eos>']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create message stack.\n",
    "message_stack = temp_message_stack.apply(\n",
    "    lambda row: GEMMA_INST.format(full_user_prompt=row[\"message\"], model=row[\"next_message\"]),\n",
    "    axis=1,\n",
    ").to_list()\n",
    "\n",
    "message_stack = pandas.DataFrame(message_stack, columns=[\"message\"])\n",
    "\n",
    "# Convert to dataset.\n",
    "training_dataset = Dataset.from_pandas(message_stack)\n",
    "\n",
    "training_dataset[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3f85d6e7348089077bc2490211394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"gemma-1.1-2b-it\" # Note that you may need access token to download the model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\":0}, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h-alice\\Documents\\Projects\\trashtalk-bot\\.conda\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:575: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell me more about the sun. \n",
      "Note: Please explain within 50 words.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The Sun is a star that shines brightly in our solar system, primarily composed of hydrogen and helium gases. It generates its energy through nuclear fusion reactions at its core, releasing light and heat into space. The Sun provides life-sustaining radiation to planets orbiting it, enabling plant growth and animal survival on Earth<eos>\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"<start_of_turn>user\n",
    "Tell me more about the sun. \n",
    "Note: Please explain within 50 words.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "input_prompt = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_prompt, max_new_tokens=100, do_sample=True, top_k=20, repetition_penalty=1.5, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GEMMA model.\n",
    "# You can reference the model's config to get the model's target modules.\n",
    "# It will be a json file with name like \"model.safetensors.index.json\" in the model's directory.\n",
    "# For more precise configuration, take a look at the model's original paper!\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n"
     ]
    }
   ],
   "source": [
    "get_peft_model(model, lora_config).print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h-alice\\AppData\\Roaming\\Python\\Python311\\site-packages\\trl\\trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecd8bc62e6744069e964b0fee953d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20754 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h-alice\\AppData\\Roaming\\Python\\Python311\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=training_dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        overwrite_output_dir=True,\n",
    "        push_to_hub=False,\n",
    "        save_steps =500,\n",
    "        warmup_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        output_dir=\"./outputs\",\n",
    "        report_to=None,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"steps\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"message\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b1fe00e8c54033a333864e81da36cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0501, 'grad_norm': 6.071439266204834, 'learning_rate': 3.6637099884303897e-06, 'epoch': 1.97}\n",
      "{'train_runtime': 263.9065, 'train_samples_per_second': 157.283, 'train_steps_per_second': 19.658, 'train_loss': 0.07424204994736953, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5188, training_loss=0.07424204994736953, metrics={'train_runtime': 263.9065, 'train_samples_per_second': 157.283, 'train_steps_per_second': 19.658, 'total_flos': 1.206500619952128e+16, 'train_loss': 0.07424204994736953, 'epoch': 1.9999036330345956})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "你在幹嘛\n",
      "<end_of_turn>\n",
      "<start_of_turn>model\n",
      "快把那把菜刀拿過來<eos>\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"<start_of_turn>user\n",
    "你在幹嘛\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "input_prompt = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_prompt, max_new_tokens=200, do_sample=True, top_k=60, top_p=0.9, repetition_penalty=1.2, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
