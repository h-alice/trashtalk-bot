{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model # PEFT model\n",
    "import torch  # PyTorch, needless to say ;)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig  # Main components of LLM\n",
    "from pathlib import Path # For file paths\n",
    "from datasets import load_dataset, Dataset # For loading the dataset\n",
    "from trl import SFTTrainer  # An easy-to-use trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_raw = load_dataset(\"h-alice/chat-cooking-master-boy-100k\", split=\"train\")\n",
    "dataset_raw = dataset_raw.map(lambda x: {\"message\": \"<bos>\" + x[\"message\"] + \"<eos>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e3adb3ef594a1d8456b04ebeb94518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = r\"google\\gemma-2b\" # Note that you may need access token to download the model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map={\"\":0}, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h-alice\\Documents\\Projects\\trashtalk-bot\\.conda\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:575: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Hi, my name is Gemma, I am a qualified Clinical Hypnotherapist and Coach. My background in the fitness industry has allowed me to build strong relationships with people from all walks of life; athletes & performers who have reached their maximum potential or those just wanting some guidance on how they can achieve that ‘light at the end’ of this very dark tunnel we are currently living through!\n",
      "\n",
      "I work as an accredited practitioner for both Mind Body Health Therapy Ltd (www.mbhtherapyltd.co.uk) – offering online coaching\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hi, my name is Gemma,\"\"\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "input_prompt = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_prompt, max_new_tokens=100, do_sample=True, top_k=20, repetition_penalty=1.5, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GEMMA model.\n",
    "# You can reference the model's config to get the model's target modules.\n",
    "# It will be a json file with name like \"model.safetensors.index.json\" in the model's directory.\n",
    "# For more precise configuration, take a look at the model's original paper!\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,497,408 || all params: 2,510,669,824 || trainable%: 0.1791317980966023\n"
     ]
    }
   ],
   "source": [
    "get_peft_model(model, lora_config).print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h-alice\\AppData\\Roaming\\Python\\Python311\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_raw,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        overwrite_output_dir=True,\n",
    "        push_to_hub=False,\n",
    "        save_steps =500,\n",
    "        warmup_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=100,\n",
    "        output_dir=\"./outputs\",\n",
    "        report_to=None,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_strategy=\"steps\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"message\",\n",
    "    max_seq_length=256, # This is super important, or else the trainer will bloat your GPU memory and die.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec12c17bb424b939ab229cb14b5c9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23724 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.054, 'train_samples_per_second': 3514712.644, 'train_steps_per_second': 439320.563, 'train_loss': 0.0, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "except ValueError:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>中華料理、日本料理、韓國料理通通都有，就是沒有屬於台灣的料理；既然這樣，今後只好自己創造。這個故事就是描寫擁有「料理天分」的熱血少年史丹利，從小立志，為全世界驕傲的台灣人，做出專屬於台灣人的料理，在圖奇締造的偉大抒情史詩<eos>\n"
     ]
    }
   ],
   "source": [
    "text = \"中華料理、\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "input_prompt = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_prompt, max_new_tokens=100, do_sample=True, top_k=60, top_p=0.9, repetition_penalty=1.2, temperature=0.7)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h-alice\\Documents\\Projects\\trashtalk-bot\\.conda\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in C:\\Users\\h-alice\\Desktop\\llms\\google\\gemma-2b - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./outputs_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
